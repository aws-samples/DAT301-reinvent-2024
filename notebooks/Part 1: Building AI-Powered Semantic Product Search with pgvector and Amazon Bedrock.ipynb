{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Building AI-Powered Semantic Product Search with pgvector and Amazon Bedrock\n",
    "### Configuration, Vector Embeddings and Data Ingestion\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "2. [Architecture](#Architecture)\n",
    "3. [Setup](#Setup)\n",
    "4. [Load Product Data](#Load-Product-Data)\n",
    "5. [Generate Embeddings](#Generate-Embeddings)\n",
    "6. [Store in PostgreSQL](#Store-in-PostgreSQL)\n",
    "\n",
    "## Background\n",
    "\n",
    "This lab demonstrates how to build a semantic product search system using vector embeddings. Key components:\n",
    "\n",
    "- **Vector Embeddings**: Using Amazon Titan Embeddings to convert product descriptions into numerical vectors that capture semantic meaning\n",
    "- **Vector Storage**: Using pgvector extension in Aurora PostgreSQL to efficiently store and search these vectors\n",
    "- **Semantic Search**: Finding similar products by comparing vector distances\n",
    "\n",
    "## Architecture\n",
    "\n",
    "1. Product descriptions are converted to embeddings using Amazon Bedrock.\n",
    "2. Embeddings are stored in Aurora PostgreSQL using the pgvector extension.\n",
    "3. Search queries are converted to embeddings and compared using vector similarity.\n",
    "4. Most similar products are returned based on vector distance.\n",
    "\n",
    "![Building AI-Powered Semantic Product Search with pgvector and Amazon Bedrock](../static/Product_Catalog.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all the required prerequiste libraries\n",
    "%pip install setuptools==65.5.0\n",
    "%pip install \"psycopg[binary]\" pgvector pandarallel boto3 tqdm numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import json\n",
    "import psycopg\n",
    "from pgvector.psycopg import register_vector\n",
    "from pandarallel import pandarallel\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize Bedrock client\n",
    "bedrock_runtime = boto3.client('bedrock-runtime')\n",
    "\n",
    "print(\"Required libraries setup complete ✅ \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aurora PostgreSQL Database Setup\n",
    "\n",
    "Set up PostgreSQL with the pgvector extension and create our product catalog table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get database credentials from Secrets Manager\n",
    "client = boto3.client('secretsmanager')\n",
    "response = client.get_secret_value(SecretId='apg-pgvector-secret-RIV')\n",
    "database_secrets = json.loads(response['SecretString'])\n",
    "\n",
    "# Set up database connection parameters\n",
    "dbhost = database_secrets['host']\n",
    "dbport = database_secrets['port']\n",
    "dbuser = database_secrets['username']\n",
    "dbpass = database_secrets['password']\n",
    "\n",
    "def setup_database():\n",
    "    \"\"\"Set up database schema and tables\"\"\"\n",
    "    conn = psycopg.connect(\n",
    "        host=dbhost,\n",
    "        port=dbport,\n",
    "        user=dbuser,\n",
    "        password=dbpass,\n",
    "        autocommit=True\n",
    "    )\n",
    "\n",
    "    # Enable vector extension\n",
    "    conn.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "    register_vector(conn)\n",
    "\n",
    "    # Create schema\n",
    "    conn.execute(\"CREATE SCHEMA IF NOT EXISTS bedrock_integration;\")\n",
    "    \n",
    "    # Drop existing table if needed\n",
    "    conn.execute(\"DROP TABLE IF EXISTS bedrock_integration.product_catalog;\")\n",
    "\n",
    "    # Create products table\n",
    "    conn.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bedrock_integration.product_catalog (\n",
    "        \\\"productId\\\" VARCHAR(255) PRIMARY KEY,\n",
    "        product_description TEXT,\n",
    "        imgUrl TEXT,\n",
    "        productURL TEXT,\n",
    "        stars NUMERIC,\n",
    "        reviews INT,\n",
    "        price NUMERIC,\n",
    "        category_id INT,\n",
    "        isBestSeller BOOLEAN,\n",
    "        boughtInLastMonth INT,\n",
    "        category_name VARCHAR(255),\n",
    "        quantity INT,\n",
    "        embedding vector(1024)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # Create HNSW index\n",
    "    conn.execute(\"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS product_catalog_embedding_idx \n",
    "    ON bedrock_integration.product_catalog \n",
    "    USING hnsw (embedding vector_cosine_ops);\n",
    "    \"\"\")\n",
    "        \n",
    "    print(f\"Connection info: host={dbhost}, port={dbport}, user={dbuser}\")\n",
    "    print(\"Database setup complete ✅\")\n",
    "    conn.close()\n",
    "\n",
    "setup_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Product Catalog Data\n",
    "\n",
    "Load and preprocess the product catalog data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load product data\n",
    "print(\"Loading product data...\")\n",
    "df = pd.read_csv('../datasets/product_catalog.csv')\n",
    "\n",
    "# Clean up missing values\n",
    "df = df.dropna(subset=['product_description'])\n",
    "df = df.fillna({\n",
    "    'stars': 0,\n",
    "    'reviews': 0,\n",
    "    'price': 0,\n",
    "    'category_id': 0,\n",
    "    'isBestSeller': False,\n",
    "    'boughtInLastMonth': 0,\n",
    "    'category_name': 'Unknown',\n",
    "    'quantity': 0\n",
    "})\n",
    "\n",
    "print(f\"Loaded {len(df)} products\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings\n",
    "\n",
    "Generate embeddings using Amazon Bedrock's Titan model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(text):\n",
    "    \"\"\"Generate embedding for a single text using Amazon Titan Text v2\"\"\"\n",
    "    try:\n",
    "        payload = json.dumps({'inputText': text})\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            body=payload,\n",
    "            modelId='amazon.titan-embed-text-v2:0',\n",
    "            accept=\"application/json\",\n",
    "            contentType=\"application/json\"\n",
    "        )\n",
    "        response_body = json.loads(response.get(\"body\").read())\n",
    "        return response_body.get(\"embedding\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Initialize parallel processing\n",
    "print(\"\\nGenerating embeddings for product descriptions...\")\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=10)\n",
    "\n",
    "# Generate embeddings\n",
    "df['embedding'] = df['product_description'].parallel_apply(generate_embedding)\n",
    "\n",
    "print(\"\\nCompleted embedding generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store in Database\n",
    "\n",
    "Store the products and their embeddings in Aurora PostgreSQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_products():\n",
    "    \"\"\"Store products in database with batch processing and statistics\"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    batch_size = 1000\n",
    "\n",
    "    conn = psycopg.connect(\n",
    "        host=dbhost,\n",
    "        port=dbport,\n",
    "        user=dbuser,\n",
    "        password=dbpass,\n",
    "        autocommit=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Storing products in database... Total rows to process: {len(df)}\")\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            batches = []\n",
    "            total_processed = 0\n",
    "            \n",
    "            # Process data in batches\n",
    "            for i, (_, row) in enumerate(df.iterrows(), 1):\n",
    "                batches.append((\n",
    "                    row['productId'],\n",
    "                    row['product_description'],\n",
    "                    row['imgUrl'],\n",
    "                    row['productURL'],\n",
    "                    row['stars'],\n",
    "                    row['reviews'],\n",
    "                    row['price'],\n",
    "                    row['category_id'],\n",
    "                    row['isBestSeller'],\n",
    "                    row['boughtInLastMonth'],\n",
    "                    row['category_name'],\n",
    "                    row['quantity'],\n",
    "                    row['embedding']\n",
    "                ))\n",
    "                \n",
    "                # When batch size is reached or at the end, process the batch\n",
    "                if len(batches) == batch_size or i == len(df):\n",
    "                    batch_start = time.time()\n",
    "                    \n",
    "                    cur.executemany(\"\"\"\n",
    "                    INSERT INTO bedrock_integration.product_catalog (\n",
    "                        \"productId\", product_description, imgUrl, productURL,\n",
    "                        stars, reviews, price, category_id, isBestSeller,\n",
    "                        boughtInLastMonth, category_name, quantity, embedding\n",
    "                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                    ON CONFLICT (\"productId\") DO UPDATE \n",
    "                    SET \n",
    "                        product_description = EXCLUDED.product_description,\n",
    "                        imgUrl = EXCLUDED.imgUrl,\n",
    "                        productURL = EXCLUDED.productURL,\n",
    "                        stars = EXCLUDED.stars,\n",
    "                        reviews = EXCLUDED.reviews,\n",
    "                        price = EXCLUDED.price,\n",
    "                        category_id = EXCLUDED.category_id,\n",
    "                        isBestSeller = EXCLUDED.isBestSeller,\n",
    "                        boughtInLastMonth = EXCLUDED.boughtInLastMonth,\n",
    "                        category_name = EXCLUDED.category_name,\n",
    "                        quantity = EXCLUDED.quantity,\n",
    "                        embedding = EXCLUDED.embedding;\n",
    "                    \"\"\", batches)\n",
    "                    \n",
    "                    total_processed += len(batches)\n",
    "                    batch_time = time.time() - batch_start\n",
    "                    elapsed_total = time.time() - start_time\n",
    "                    \n",
    "                    # Calculate progress and estimated time remaining\n",
    "                    progress = (total_processed / len(df)) * 100\n",
    "                    avg_time_per_batch = elapsed_total / (total_processed / batch_size)\n",
    "                    remaining_batches = (len(df) - total_processed) / batch_size\n",
    "                    eta = remaining_batches * avg_time_per_batch\n",
    "                    \n",
    "                    print(f\"\\rProgress: {progress:.1f}% | Processed: {total_processed}/{len(df)} rows | \"\n",
    "                          f\"Batch time: {batch_time:.2f}s | ETA: {eta:.0f}s\", end=\"\")\n",
    "                    \n",
    "                    batches = []\n",
    "            \n",
    "            print(\"\\n\\nRunning VACUUM ANALYZE...\")\n",
    "            cur.execute(\"VACUUM ANALYZE bedrock_integration.product_catalog;\")\n",
    "            \n",
    "            # Get final statistics\n",
    "            cur.execute(\"SELECT COUNT(*) FROM bedrock_integration.product_catalog\")\n",
    "            final_count = cur.fetchone()[0]\n",
    "            \n",
    "            end_time = time.time()\n",
    "            total_time = end_time - start_time\n",
    "            \n",
    "            print(\"\\n📊 Data Loading Statistics:\")\n",
    "            print(f\"✓ Total rows loaded: {final_count:,}\")\n",
    "            print(f\"✓ Total loading time: {total_time:.2f} seconds\")\n",
    "            print(f\"✓ Average time per row: {(total_time/len(df))*1000:.2f} ms\")\n",
    "            print(f\"✓ Average time per batch: {(total_time/(len(df)/batch_size)):.2f} seconds\")\n",
    "            print(\"\\n✅ Products stored successfully in database\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error storing products: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Load data with embeddings into the table\n",
    "store_products()\n",
    "print(\"\\nPart 1 Complete: Setup and data loading finished! ✅\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
